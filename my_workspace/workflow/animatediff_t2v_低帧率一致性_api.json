{
  "2": {
    "inputs": {
      "vae_name": "vae-ft-mse-840000-ema-pruned.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "åŠ è½½VAE"
    }
  },
  "3": {
    "inputs": {
      "text": "(1girl:1.2), young woman with nostalgic expression, wandering alone on a dimly lit urban street, old neon signs flickering in the background, soft warm glow from vintage street lamps, autumn leaves gently drifting in the air, light rain making the pavement shimmer, distant silhouettes of people lost in thought, sepia-toned atmosphere, melancholic yet hopeful gaze, slightly messy hair caught in the breeze, wearing a worn-out trench coat, subtle film grain effect, cinematic composition, atmospheric perspective\n",
      "clip": [
        "4",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIPæ–‡æœ¬ç¼–ç "
    }
  },
  "4": {
    "inputs": {
      "stop_at_clip_layer": -2,
      "clip": [
        "48",
        1
      ]
    },
    "class_type": "CLIPSetLastLayer",
    "_meta": {
      "title": "è®¾ç½®CLIPæœ€åä¸€å±‚"
    }
  },
  "6": {
    "inputs": {
      "text": "(worst quality, low quality: 1.4)",
      "clip": [
        "4",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIPæ–‡æœ¬ç¼–ç "
    }
  },
  "7": {
    "inputs": {
      "seed": 1105201443598834,
      "steps": 4,
      "cfg": 2,
      "sampler_name": "lcm",
      "scheduler": "normal",
      "denoise": 1,
      "model": [
        "40",
        0
      ],
      "positive": [
        "3",
        0
      ],
      "negative": [
        "6",
        0
      ],
      "latent_image": [
        "9",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "Ké‡‡æ ·å™¨"
    }
  },
  "9": {
    "inputs": {
      "width": 768,
      "height": 432,
      "batch_size": 16
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "ç©ºLatentå›¾åƒ"
    }
  },
  "10": {
    "inputs": {
      "samples": [
        "49",
        0
      ],
      "vae": [
        "2",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAEè§£ç "
    }
  },
  "30": {
    "inputs": {
      "ckpt_name": "nostalgiaClear_nostalgiaClear.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "CheckpointåŠ è½½å™¨ï¼ˆç®€æ˜“ï¼‰"
    }
  },
  "34": {
    "inputs": {
      "frame_rate": 4,
      "loop_count": 0,
      "filename_prefix": "aaa_readme_cn",
      "format": "video/h264-mp4",
      "pix_fmt": "yuv420p",
      "crf": 19,
      "save_metadata": true,
      "trim_to_audio": false,
      "pingpong": false,
      "save_output": true,
      "images": [
        "10",
        0
      ]
    },
    "class_type": "VHS_VideoCombine",
    "_meta": {
      "title": "Video Combine ğŸ¥ğŸ…¥ğŸ…—ğŸ…¢"
    }
  },
  "36": {
    "inputs": {
      "batch_offset": 0,
      "noise_type": "FreeNoise",
      "seed_gen": "comfy",
      "seed_offset": 0,
      "adapt_denoise_steps": 0
    },
    "class_type": "ADE_AnimateDiffSamplingSettings",
    "_meta": {
      "title": "Sample Settings ğŸ­ğŸ…ğŸ…“"
    }
  },
  "37": {
    "inputs": {
      "context_length": 16,
      "context_overlap": 4,
      "fuse_method": "pyramid",
      "use_on_equal_length": false,
      "start_percent": 0,
      "guarantee_steps": 1
    },
    "class_type": "ADE_StandardStaticContextOptions",
    "_meta": {
      "title": "Context Optionsâ—†Standard Static ğŸ­ğŸ…ğŸ…“"
    }
  },
  "40": {
    "inputs": {
      "model_name": "AnimateLCM_sd15_t2v.ckpt",
      "beta_schedule": "autoselect",
      "model": [
        "48",
        0
      ],
      "context_options": [
        "37",
        0
      ],
      "sample_settings": [
        "36",
        0
      ]
    },
    "class_type": "ADE_AnimateDiffLoaderGen1",
    "_meta": {
      "title": "AnimateDiff Loader ğŸ­ğŸ…ğŸ…“â‘ "
    }
  },
  "48": {
    "inputs": {
      "lora_name": "AnimateLCM_sd15_t2v_lora.safetensors",
      "strength_model": 1,
      "strength_clip": 1,
      "model": [
        "30",
        0
      ],
      "clip": [
        "30",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "åŠ è½½LoRA"
    }
  },
  "49": {
    "inputs": {
      "seed": 652377657084102,
      "steps": 4,
      "cfg": 2,
      "sampler_name": "lcm",
      "scheduler": "normal",
      "denoise": 1,
      "model": [
        "40",
        0
      ],
      "positive": [
        "3",
        0
      ],
      "negative": [
        "6",
        0
      ],
      "latent_image": [
        "50",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "Ké‡‡æ ·å™¨"
    }
  },
  "50": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "scale_by": 2.0000000000000004,
      "samples": [
        "7",
        0
      ]
    },
    "class_type": "LatentUpscaleBy",
    "_meta": {
      "title": "ç¼©æ”¾Latentï¼ˆæ¯”ä¾‹ï¼‰"
    }
  }
}